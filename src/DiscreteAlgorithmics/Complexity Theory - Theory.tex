Complexity theory is an important domain of theoretical computer science, the most famous problem of which is probably the \og $ \mathsf P = \mathsf{NP} $ \fg problem. It asks whether or not $ \mathsf{NP} $ problems are in $ \mathsf P $. Though we can, for many problems, proove that they \textit{are indeed} in a given problem complexity class, it is always much harder to proove that a given problem is \textit{not} in some class.

We present the general theory, investigate the definitions along with their philosophical meanings, explore different complexity classes (co-$ \mathsf{NP} $, $ \mathsf{NPC} $, $ \mathsf{NPH} $, $ \mathsf{PH} $, ...) and their relationships. Next, we consider various algorithmic problems, study the relationships and reductions between them and some algorithms to solve them.

The references for this chapter are : Hudry's introductions \cite{charonhudry2019}, \cite{hudry2024}, Gowers' course \cite{gowers2023}, \cite{gowers2024}, which are based on \cite{} , besides thoses cited herein.

\section{Theory}

\subsection{Turing machines and complexity}

\subsubsection{Decision problems}

\begin{definition}[Boolean function]
    Throughout the chapter, we will denote
    \[
    \{0,1\}^* = \bigcup_{n=1}^\infty \{0,1\}^n
    \]
    the set of finite length binary strings.
    A \textit{boolean function} is a function
    \[
    \phi : \{0,1\}^* \to \{0,1\}
    \]
    A \textit{decision problem} is a boolean function.
\end{definition}

In his course on the theory of complexity, Timothy Gowers \cite{gowers2023}, \cite{gowers2024} defines \textit{problems} formally in such way : as boolean functions. After a small think, we get easily convinced that this definition is very good and manipulatable, especially regarding the formalism of Turing machines that will come next.

In case this was not clear enough, a decision problem is a problem that asks a question, the answer of which must be either \og Yes \fg or \og No \fg. Then, it becomes cristal clear that any (worthy of interest) mathematical decision problem can me encoded as a bÂ²oolean function. For instance, the hamiltonian cycle is one of such.

\begin{definition}[Hamiltonian path]
    Let $ G = (V,E) $ be a graph. A \textit{hamiltonian cycle} or \textit{hamiltonial path} in $ G $ is a cycle in $ G $ that travels all vertices of $ G $ exactly once, and all edges of $ G $ at most once.
\end{definition}

\begin{problem}
    \textbf{Problem :} Hamiltonian path (\textsc{Ham})

    \textbf{Input :} A graph $ G = (V,E) $

    \textbf{Output :} \og Yes \fg if it is possible to find a hamiltonian path in $ G $, and \og No \fg otherwise.
\end{problem}

A graph $ G = (V,E) $ can be encoded as an element of $ \{ 0,1 \}^* $ of length $ |E|^2 $ (or actually $ |E|^2 - |E| $, if one considers the mostly standard definition of a graph that disallows loops), by agreeing on a bijection between the slots and the pairs of (distinct) vertices, and placing a $ 1 $ at slots that are connected by an edge and $ 0 $ on the others.

In the case of the hamiltonian path problem, $ \phi $ is the function defined on $ \{ 0,1 \}^* $ that returns \og 1 \fg on graphs that contain a hamiltonian path, and \og 0 \fg that don't. (For entries that do not represent graphs, whatever, just say that $ \phi $ outputs 0.)

\subsubsection{(Deterministic) Turing machines and computability}

Boolean functions are made to be processed by \textit{Turing machines}.

\begin{definition}[Deterministic Turing machine]
    A \textit{deterministic Turing machine} is a triple $ (\mathcal A, \Sigma, \delta) $, where :
    \begin{itemize}
        \item $ \mathcal A $ is a finite set, called the \textit{alphabet} of the Turing machine.
        \item $ \Sigma $ is a finite set, the elements of which are called the machine's \textit{states}.
        \item $ \delta $ is the \textit{transition function}, and is a map
        \[
        \delta : X \subset \mathcal A \times \Sigma \to \mathcal A \times \Sigma \times \{ \texttt{L}, \texttt{R}\}
        \]
    \end{itemize}
\end{definition}

Deterministic Turing machines operate on an infinite tape divided into cells, each capable of holding a symbol from the alphabet $ \mathcal A $. $ \Sigma $ represents the (finite) set of internal states, one of which is designated as the initial state. The behavior of the machine is then entirely governed by the transition function $ \delta $, which specifies the machine's actions based on the current state and the symbol it reads from the tape. More precisely, $ \delta $ maps a subset of $ \mathcal A \times \Sigma $, to the set of triples $ (a, q, d) \in \mathcal A \times \Sigma \times \{ \texttt L, \texttt R \} $. Each step, the machine reads the current symbol $ x $, and according to its state $ s \in \Sigma $, it computes the output $ \delta(x,s) = (a, q, d) $. Here, $ a \in \mathcal A $ is the symbol that will replaces the current tape symbol $ x $, the machine will write it on the tape. The state will then change from $ s $ to $ q $, and the machine will move on the tape in the direction $ d \in \{ \texttt L, \texttt R\} $. This process continues until the machine reaches a symbol-state pair not in the domain of $ \delta $, at which point it halts.

Turing machines were introduced by A. Turing \cite{turing1936}, \cite{turing1992} to propose a model for \textit{computability}. They provide a theoretical computational model that represents \textit{algorithms}. They answer the question : \textit{what do we mean exactly by an algorithm ? What is computable ?} 

Let us give some examples : the sequence $ 0101010101... $ is \textit{computable} in the sense that there is an obvious pattern to it, and as such, it is possible for an algorithm, given any positive integer $ n $, to write its first $ n $ terms. This property also holds for the sequence $ 01001000100001 \dots $. However, if I choose \og random \fg infinite sequence $ S = 010010101010010101111101... $, it might be that there is no \textit{pattern} to it, or any \textit{way to describe a method to compute its $ n $-th term}. This property, \og there exists no method to describe $ S $ \fg, or more precisely, \og there exists no algorithm (that can be written in a finite amount of space) that computes $ S $ \fg, is what makes a sequence $ S $ \textit{uncomputable}.

Turing machines are not the only approach to define computability. \textit{$ \lambda $-calculus} and \textit{general recursive functions} are two other approches that have been proposed. It is possible, for each of them, to define which functions are computable and which are not. We then have the following theorem \cite{soare1996}, \cite{church1936}, \cite{kleene1936}, \cite{turing1937} :

\begin{theorem}[Church-Turing thesis]
    The functions that are \textit{computable} respectively in the sense of Turing machines, $ \lambda $-calculus, and general recursive functions, are all the same. 
    
    In other words, these three methods give birth to the exact same set of functions. 
\end{theorem}

This is known as the Church-Turing thesis.

\begin{definition}[Computable function]
    We call \textit{computable functions}, the functions that can, equivalently, be computed either in the sense of Turing machines, $ \lambda $-calculus or general recursive functions.
\end{definition}

Turing Machines are also an extremely robust model, in the sense that most non-corebreaking variations on their definition leave absolutely unchanged the induced set of \textit{computable functions}. For instance, it is possible to replace the infinite-length tape by a semi-infinite one, or by two (with two heads reading in parallel), or three, or any finite number. You can replace it by $k \in \mathbf N^*$ semi-infinite ones, that all meet at the same point. You can change the number of symbols to any arbitrary finite cardinality, and so on and so forth.

\subsubsection{(Deterministic) Turing machines as a model for complexity}

After having been defined as a computability model, Turing Machines have been used to formally define the \textit{complexity} of an algorithm. This will be done right afterwards, but we must make some other considerations first.

When we wish to define precisely the \emph{size} of a problem instance (that is, the total set of data describing the problem), it is important to account for all the data. For example, in the shortest path problem, one must consider not only the number of vertices or arcs, but also the weights of the arcs.

As mentionned earlier, it is clear that this can always be done as a string in $ \{0,1\}^* $. More generally, the description of a problem instance can be seen as a finite string of characters belonging to a fixed alphabet \( \mathcal A \). We here discuss the importance (or, shall we say, the non-importance) of the cardinality of \( \mathcal A \) - and attempt to explain why such non-importance. Given a fixed encoding of the data, the size of an instance \( I \) is defined as the length of the character string that describes \( I \), i.e., the number of symbols from \( \mathcal A \) required to describe \( I \). If \( a \) denotes the cardinality of \( \mathcal A \), then the size of \( I \) with respect to \( \mathcal A \) is denoted \( |I|_a \), or simply \( |I| \) if no ambiguity arises. Note : In this chapter, the notation \( |\cdot| \) refers to instance size, not cardinality.

The binary alphabet \( \{0, 1\} \) is often used to encode data; in that case, we refer to \emph{binary size}. With this alphabet:
\begin{itemize}
    \item Representing a non-negative integer \( k \) requires \( \lceil \log_2(k+1) \rceil \) bits;
    \item Representing a graph with \( n \) vertices using its adjacency matrix requires \( n^2 \) bits;
    \item Representing a weighted graph using its valuation matrix \( (v_{i,j}) \) requires
    \[
        \sum_{i,j} \lceil \log_2(v_{i,j} + 1) \rceil
    \]
    bits, assuming all \( v_{i,j} \in \mathbf N \).
\end{itemize}

Thus, in binary encoding, the size of an instance is the number of bits needed to represent all defining data : numbers, sets, graphs, or other structures.

Later, we will also consider \emph{unary encoding} for representing integers. Unary encoding describes a strictly positive integer \( n \) using \( n \) ``sticks''; it can be adapted for signed integers. The unary size \( |n|_1 \) is \( n \), while the binary size \( |n|_2 \) is of order \( \log_2 n \), showing that these two sizes are not polynomially related.

On the other hand, consider an alphabet \( \mathcal A \) with \( a > 2 \) symbols. Then the binary size \( |I|_2 \) and the size \( |I|_a \) of a description of an instance \( I \) using \( \mathcal A \) satisfy:
\[
|I|_a < |I|_2 < \lceil \log_2(a) \cdot |I|_a \rceil,
\]
showing the two sizes are polynomially related. Since the set of polynomials is closed under standard operations (including composition), results obtained using binary encodingâsuch as membership in complexity classes \( \mathsf{P} \), \( \mathsf{NP} \), or \( \mathsf{NP} \)-completenessâalso hold under any encoding using \( \mathcal A \).

Therefore, unless we explicitly discuss unary encoding, we will assume throughout this chapter that binary encoding is used: all data is encoded using 0 and 1. On a side note, \cite{charonhudry2019} considers a slight variant of this encoding : it allows the usage of a third symbol, denoted $ b $ (that stands for something like \og blank \fg or \og blank space \fg), that is used as a separator symbol for the data. This is thus just another encoding using another alphabet (of cardinality 3).

Unless stated otherwise, graphs will be encoded by their adjacency matrix, or by their valuation matrix if they are weighted. Consequently, the encoding of a graph of order \( n \) has size at least \( n^2 \).

We can now give a precise definition of the complexity of an algorithm, via a Turing machine modeling this algorithm. Let $\Pi$ be a problem and let $I$ be an arbitrary instance of $\Pi$, of size $|I|$. Let $A$ be an algorithm solving $\Pi$, and let $M_A$ be a Turing machine associated with $A$. When we apply $A$ to $I$, $M_A$ performs a certain number of steps $\phi_A(I)$ (in other words, we apply the transition function $\tau$ of $M_A$ a number of times equal to $\phi_A(I)$ before halting); throughout this chapter, we limit ourselves to cases where $\phi_A(I)$ is finite for every instance $I$ (which excludes, for example, certain methods from Chapters \ref{chap:heuristics} and \ref{chap:heuristics}). To define the complexity $\gamma_A$ of $A$ (or of $M_A$), we group all instances $I$ of the same size $k$ and consider the largest value of $\phi_A(I)$ over this set of instances:
\[
\gamma_A(k) = \max\{\phi_A(I) \mid |I| = k\}.
\]
Taking the time needed to perform one step of a Turing machine as the time unit, $\phi_A(I)$ measures the computation time taken by $A$ to process $I$, and $\gamma_A(k)$ gives an upper bound on the computation time needed to solve an instance of size $k$ using $A$.

In practice, this definition of algorithm complexity is replaced by a more operational notion, relying on the concept of an \textit{elementary operation}, i.e., one of the following operations\footnote{The listed operations are considered atomic as long as the operands are of "simple" type.}:
\begin{itemize}
    \item arithmetic operations (addition, subtraction, multiplication, division, quotient or remainder in integer division),
    \item comparison operations ($=, \neq, <, >, \leq, \geq$),
    \item standard boolean operations (or, and, not),
    \item memory access for reading or writing (assignment),
\end{itemize}
when the operands are of a \textit{simple} type (integers, reals, booleans, characters, but not vectors, matrices, or strings).

We then adapt what was presented above to define the complexity of an algorithm, but now based on elementary operations rather than Turing machine steps. This gives the following definition:

\begin{definition}[Complexity]
    Let $\Pi$ be a problem, let $I$ be an arbitrary instance of $\Pi$, of size $|I|$, and let $A$ be an algorithm solving $\Pi$. When we apply $A$ to $I$, $A$ performs a certain number of elementary operations $\psi_A(I)$. The (worst-case) complexity of $A$ is the function $\gamma_A$ defined from $\mathbf{N}$ to $\mathbf{N}$ by:
    \[
    \gamma_A(k) = \max\{\psi_A(I) \mid |I| = k\}.
    \]
\end{definition}

This notion of complexity, based on counting elementary operations, is the one used a bit everywhere. It has the advantage of not requiring formalization of the algorithm via a Turing machine and allows for the use of more familiar concepts without affecting qualitative results. We will follow the same approach going forward: each assignment, arithmetic operation, comparison, or boolean operation on simple types counts as one unit.

If we assume that each elementary operation takes one time unit (which is a simplification: for example, $p \times q$ generally takes more time than $p + q$), the complexity remains an estimate of the computation time. It is therefore still a time complexity.

\begin{definition}[Polynomial algorithm]
    An algorithm $A$ is said to be \textit{polynomial} if its complexity $\gamma_A$ can be upper bounded by a polynomial $P$ in the binary size $|I|_2$ of the instance $I$ being processed.
\end{definition}

\begin{definition}[Pseudo-polynomial algorithm]
    An algorithm $A$ is said to be \textit{pseudo-polynomial} if its complexity $\gamma_A$ can be upper bounded by a polynomial $P$ in the unary size $|I|_1$ and the binary size $|I|_2$ of the instance $I$ being processed (or, equivalently, by a polynomial in $|I|_1$, since $|I|_2 \leq |I|_1$).
\end{definition}

It follows from these definitions that :

\begin{proposition}
    A polynomial algorithm is also pseudo-polynomial.
\end{proposition}

Saying that an algorithm $A$ is polynomial does not mean that its complexity $\gamma_A$ is a polynomial, but only that it is \textit{bounded above} by a polynomial. For example, a binary search algorithm in a sorted array of $k$ integers has complexity $O(\log k)$; it is polynomial, because the binary size of the $k$ integers is at least $k$ (one bit per integer).

An algorithm whose complexity cannot be bounded by a polynomial is sometimes said to be \textit{exponential}, even if its complexity is not an exponential function in the strict sense. For example, a complexity of $k!$ is not bounded above by a polynomial in $k$ (binary size of the instance), even though the factorial is not an exponential.

Polynomial algorithms are sometimes called \textit{good algorithms} or \textit{efficient algorithms}. However, in practice, some exponential algorithms can perform well (for example, the simplex algorithm for linear optimization problems: its worst-case complexity is exponential, but its average complexity is polynomial under certain data distribution assumptions).

Moreover, an algorithm with complexity $n^{1.000001}$ is faster than another with complexity $n$ for $n < 16\,626\,517$.

The complexity $\gamma_A$ is generally expressed using Landau's notation $O$ and is always an increasing function.

\subsubsection{Polynomial reducibility}

We conclude with the notion of (polynomial) reducibility. 

\begin{definition}[Polynomial reducibility]
    Let $ \phi $ and $ \psi $ be two decision problems. We say that $ \gamma : \{0,1\}^* \to \{0,1\}^* $ is a \textit{reduction} from $ \phi $ to $ \psi $ if 
    \[ 
        \forall x \in \{0,1\}^*, \phi(x) = \psi(\gamma(x))
    \]
    The reduction is said to be \textit{polynomial} if it can be computed in polynomial time.
\end{definition}

\begin{definition}
    We will denote $ \prec $ the binary relation of polynomial reducibility.
\end{definition}

In other words, $ A \prec B $ if the problem $ A $ is reducible to $ B $.

\begin{proposition}
    The binary relation $ \prec $ is reflexive and transitive : it is a preorder in the set of decision problems.
\end{proposition}

\begin{proof}
    Taking $ \gamma (x) = x $ in the definition of polynomial reducibility yields reflexivity. If $ P $ and $ Q $ are polynomials, then $ P(Q) $ is still a polynomial, whence transitivity.
\end{proof}

The polynomial reducibility relation $ \prec $ is interpreted by saying that $ a \prec b $ if $ b $ is \textit{computationally harder} than $ a $. For instance, we trivially have $ \mathsf{3SAT} \prec \mathsf{SAT} $. We will also see in the following that $ \mathsf{SAT} \prec \mathsf{3SAT} $ : $ \mathsf{3SAT} $ is thus also computationally harder than $ \mathsf{SAT} $, which is quite surprising at first glance.

\subsection{Different complexity classes}

\subsubsection{The $ \mathsf P $ class}

We can now define the class $ \mathsf P $ of polynomial problems.

\begin{definition}[Polynomial problem]
    A problem is said to be \textit{polynomial} if there exists a polynomial algorithm to solve it.

    The class of polynomial problems is denoted $ \mathsf P $.
\end{definition}

Recall that we had only defined polynomial \textit{algorithms} and not \textit{problems}. Do not confuse both notions.

In other words, $ \mathsf P $ is the set of boolean functions $ \phi $, for which there exists a Turing machine that solves $ \phi $ in a polynomial number of steps. Recall that this definition does not depend on whether we consider Turing machine steps or elementary operations because Turing machine steps and elementary operations are polynomially related. We will not repeat this consideration for each new definition of a problem class set.

There is a variant of this class which contains so-called \textit{pseudo-polynomial} problems.

\begin{definition}[Pseudo-polynomial problem]
    A problem is said to be \textit{pseudo-polynomial} if there exists a pseudo-polynomial algorithm to solve it.
\end{definition}

There does not seem to be any usual notation for psuedo-polynomial problems. As with algorithms, there is a relationship between polynomial and pseudo-polynomial problems.

\begin{proposition}
    Polynomial problems are also pseudo-polynomial.
\end{proposition}

\subsubsection{The $ \mathsf{NP} $ class}

This is the definition of $ \mathsf{NP} $ problems that is given in \cite{gowers2023} and \cite{gowers2024}.

\begin{definition}\label{def:np-problem}
    We say that a decision problem $ \phi $ is a \textit{non-deterministic polynomial} problem if there exists a polynomial $ P $ and a polynomial problem $ \psi \in \mathsf P $
    \[
        \forall x \in \{ 0,1 \}^*, \phi(x) = 1 \iff \exists y \in \{ 0,1 \}^{P(n)}, \psi(x,y) = 1
    \]
    We denote $ \mathsf{NP} $ the class of non-deterministic polynomial problems.
\end{definition}

This definition is equivalent to the definition of $ \mathsf{NP} $ problems using non-deterministic Turing machines \cite{hudry2024}, \cite{charonhudry2019}, \cite{gowers2023}, \cite{gowers2024}. Non-deterministic Turing machines themselves have several equivalent definitions, let us start by the one given in \cite{gowers2024}.

\begin{definition}[Non-deterministic Turing machine, Definition 1]
    A \textit{non-deterministic Turing machine} is a quadruple \( (\mathcal A, \Sigma, \delta_0, \delta_1 ) \), where :
    \begin{itemize}
        \item $ \mathcal A $ is a finite set, called the \textit{alphabet} of the Turing machine.
        \item $ \Sigma $ is a finite set, the elements of which are called the machine's \textit{states}.
        \item $ \delta_0 $ and $ \delta_1 $ are two maps
        \[
        \delta_0, \delta_1 : X \subset \mathcal A \times \Sigma \to \mathcal A \times \Sigma \times \{ \texttt{L}, \texttt{R}\}
        \]
        which are called the Turing machine's \textit{transition functions}.
    \end{itemize}
\end{definition}

Here, the idea is that a non-deterministic Turing machine works in the same way as a deterministic one up to the following difference : when reading a symbol on the tape in a given state, the Turing machine has the choice to either \og activate \fg (or \textit{run}) the transition function $ \delta_0 $ or $ \delta_1 $. As a result, an input tape for a given Turing machine results in several possible outputs : a non-deterministic Turing machine does not output one tape, but rather, defines a way to compute several possible outputs. The core and essential idea behind NDTM is not that they should \textit{compute} anything (we already have deterministic Turing machines for that, they already define a well-established model for computability, which is robust enough to match the one provided by lambda-calculus and general recursive functions --- so in other words, they need not to be replaced), but rather, \textit{check the solution} to a problem --- this will appear clearer as we get into more details on this topic later.

Another proof of \textit{robustness} of this model, is that this definition does not change by letting the number of transition functions vary from $ 2 $ to any finite number greater than $ 2 $. Of course, going from $ 2 $ to $ 1 $ changed everything --- we then roll back to deterministic Turing machines.

\begin{definition}[Non-deterministic Turing machine, Definition 2]
    A \textit{non-deterministic Turing machine} is (also) a finite tuple \( (\mathcal A, \Sigma, \delta_1, ..., \delta_n ) \), where :
    \begin{itemize}
        \item $ \mathcal A $ is a finite set, called the \textit{alphabet} of the Turing machine.
        \item $ \Sigma $ is a finite set, the elements of which are called the machine's \textit{states}.
        \item $ (\delta_i)_{1 \leq i \leq n} $ are maps
        \[
        \delta_i : X \subset \mathcal A \times \Sigma \to \mathcal A \times \Sigma \times \{ \texttt{L}, \texttt{R}\}
        \]
        which are the Turing machine's \textit{transition functions}.
    \end{itemize}
\end{definition}

\begin{definition}[Non-deterministic Turing machine, Definition 3]
    A \textit{non-deterministic Turing machine} is (also) a finite tuple \( (\mathcal A, \Sigma, \delta) \), where :
    \begin{itemize}
        \item $ \mathcal A $ is a finite set, called the \textit{alphabet} of the Turing machine.
        \item $ \Sigma $ is a finite set, the elements of which are called the machine's \textit{states}.
        \item $ \delta $ is a map
        \[
        \delta : X \subset \mathcal A \times \Sigma \to 2^{\mathcal A \times \Sigma \times \{ \texttt{L}, \texttt{R}\} }
        \]
        which is the Turing machine's \textit{transition function}.
    \end{itemize}
\end{definition}

This time, the Turing machine is allowed to \og choose \fg any of the outputs of the transition function.

This model represents what an algorithm is able to do when working with an oracle. An oracle in algorithmics is an theoretical entity that is assumed to answer questions, or just give away any kind of information in general. In particular, regarding the definition of the $ \mathsf{NP} $ class : Turing machines are machines that can solve problems by having an oracle giving them the solution : all there's left for them to check is to proove that the solution is correct or not. The class of $ \mathsf{NP} $ is the class of problems for which it is possible to check that a given solution works in polynomial time. Non-deterministic Turing machines are then the ones that will \og solve \fg them in polynomial time - the oracle corresponds to who gives the solution.

\begin{definition}[$ \mathsf{NP} $]
    The class $ \mathsf{NP} $ of \textit{non-deterministic polynomial} (decision) problems, is the class of decision problems for which if the answer is \og yes \fg, it can be answered in polynomial time by a non-deterministic Turing machine.
\end{definition}

\begin{proposition}
    The latter definition of $ \mathsf{NP} $ problems is equivalent to the following : we have
    \begin{multline*}
        \mathsf{NP} = \{ \phi : \{ 0,1 \}^* \to \{ 0,1 \}, \exists \psi \in \mathsf P, \exists r \in \mathbf R[X], \\
         \forall x \in \{ 0,1 \}^*, \phi(x) = 1 \iff \exists y \in \{ 0,1 \}^{r(n)}, \psi(x,y) = 1 \}
    \end{multline*}
\end{proposition}

In other words, and looking back at definition \ref{def:np-problem}, $ \mathsf{NP} $ problems are the ones for which it is possible to check that the answer is \og yes \fg by checking that a solution works, in polynomial time. Here, $ x $ encodes the input of the problem and $ y $ the solution to the problem. The polynomial $ r $ is quite crucial in this definition : the solution must have a polynomially big encoding in the size of the input encoding, otherwise, it wouldn't make much sense to allow solutions to be exponentially big but treated in polynomial time.

\begin{proposition}
    \textsc{Ham} belongs to $ \mathsf{NP} $.
\end{proposition}

\begin{proof}
    Let $ G = (V,E) $ be a graph that contains a hamiltonian path. This hamiltonian cycle clearly has a (an encoding of) \og size \fg that is (can be bounded by a) polynomial in the size of (the encoding of) the input $ x = G $. More over, the algorithm ($ \phi $, to remain consistent in out notations) that checks whether or not a given path is hamiltonian is easily polynomial in time : the number of edges and vertices are clearly polynomial in the size of the input, and it is clear that checking self-avoidance and covering is also polynomial in time. Considering the last definition of non-deterministic polynomial problems, this achieves the proof that \textsc{Ham} $ \in \mathsf{NP} $.
\end{proof}

\subsubsection{The $ \mathsf{coNP} $ class}

It is worth noting that the answers \og yes \fg and \og no \fg play substantially assymetric roles in the definition of the $ \mathsf{NP} $ : whence the definition of the $ \mathsf{coNP} $ class.

\begin{definition}[$ \mathsf{coNP} $]
     The $ \mathsf{coNP} $ class is the class of problems for which, if the answer is \og no \fg, it is possible to answer it in polynomial time using a non-deterministic Turing machine.
\end{definition}

In other words, 

\begin{proposition}
    We have 
    \[
        \mathsf{coNP} = \{ 1 - \phi, \phi \in \mathsf{NP} \}.
    \]
\end{proposition}

\begin{proposition}
    We also have
    \[
        \mathsf{coNP} = \{ \phi, \exists \psi \in \mathsf P, \exists r \in \mathbf R[X], (\phi(x) = 1 \iff \forall y \in \{ 0,1 \}^{r(|x|)}, \psi(x,y) = 1 ) \},
    \]
    or, in a perhaps clearer way, 
    \[
        \mathsf{coNP} = \{ \phi, \exists \psi \in \mathsf P, \exists r \in \mathbf R[X], (\phi(x) = 0 \iff \exists y \in \{ 0,1 \}^{r(|x|)}, \psi(x,y) = 0 ) \}.
    \]
\end{proposition}

\begin{example}
    Consider the \textsc{Ham} problem : its \og co \fg version would be

    \begin{problem}
        \textbf{Problem :} $ \mathsf{co} $-\textsc{Ham}

        \textbf{Input :} A finite graph $ G $

        \textbf{Output :} \og Yes \fg is there is no hamiltonial path in $ G $ and \fg No \og otherwise. In other words, an answer to the question \og Does $ G $ \fg contain no hamiltonian path ?
    \end{problem}

    In the last form of the definition of $ \mathsf{co-NP} $, $ \psi $ would be the algotithm that takes as input a candidate hamiltonian path, and that answers \og No \fg of the candidate is a hamiltonian path in $ G $, and \og Yes \fg otherwise.

    As a result : $ \phi(x) $ is $ 1 $ if and only if, there exists no hamiltonian path : in other words, if and only if \textit{for all} candidate $ y $, $ \psi(x,y) $ is $ 1 $. And, $ \phi(x) $ is $ 0 $ if and only if, there exists a hamiltonian path : in other words, if and only if \textit{there exists} a candidate $ y $, for which $ \psi(x,y) $ is $ 0 $.
\end{example}

We may also do the same construction with polynomial problems : we have said that $ \mathsf P $ was the class of polynomial problems which were decidable in polynomial time (with a deterministic Turing machine). There is a \og similar \fg assymetry between the answers \og yes \fg and \og no \fg in the sense that we could have defined $ \mathsf P $ as the class of problems for which it is possible to know if the answer is \og yes \fg in polynomial (deterministic) time, and then define :

\begin{definition}[$ \mathsf{coP} $]
    The class $ \mathsf{coP} $ is the class of polynomial problems for which it is possible to know if the answer is \og no \fg in polynomial deterministic time.
\end{definition}

But, it would then be clear that 

\begin{proposition}
    We have $ \mathsf P = \mathsf{coP} $.
\end{proposition}

\begin{proof}
    Consider a problem for which it is possible to know if the answer is \og yes \fg in polynomial time. Then, we would know if the answer is \og no \fg in this same polynomial time. Conversely, if it is possible to know in polynomial time if the answer is \og no \fg, then it is possible to know if the answer is \og yes \fg in polynomial time.
\end{proof}

\begin{proposition}
    We also have $ \mathsf P \subset \mathsf{coNP} $, or if we prefer $ \mathsf{coP} \subset \mathsf{coNP} $.
\end{proposition}

\begin{proof}
    We have $ \mathsf P = \mathsf{coP} $. If it is possible for a determinstic Turing machine to answer in polynomial time \og Yes \fg, or equivalently \og No \fg, to a given decision problem $ \phi $, then it is definitely possible, if the answer is \og No \fg, for a non-deterministic one, to answer \og No \fg. This prooves $ \mathsf{coP} \subset \mathsf{coNP} $.
\end{proof}

\subsubsection{The polynomial hierarchy}

We will now define the so-called \textit{polynomial hierarchy}. It is a generalization of $ \mathsf P $ and $ \mathsf{NP} $ problems.

\begin{notation}
    From now on, the variable $ r $ will denote a polynomial, and we will omit the (heavy) \og $ \exists r \in \mathbf R[X] $ \fg for the (lighter) $ \exists r $. In other words, in the following, any occurence of $ \exists r $ stands for \og there exists a polynomial $ r $ \fg.

    We will also contract the \og $ \phi : \{ 0,1 \}^* \to \{ 0,1 \} $ \fg's to simple $ \phi $'s.
\end{notation}

\begin{definition}
    Set
    \[
        \mathsf \Sigma_0 = \mathsf \Pi_0 = \mathsf P,
    \]
    or, if we prefer,
    \[
        \mathsf \Sigma_0 = \mathsf P, \qquad \mathsf \Pi_0 = \mathsf{coP},
    \]
    and
    \[
        \mathsf \Sigma_1 = \mathsf{NP}, \qquad \mathsf \Pi_1 = \mathsf{coNP}.
    \]
    Then, inductively,
    \[
        \mathsf \Sigma_{k+1} = \{ \phi, \exists \psi \in \mathsf \Pi_k, \exists r, (\phi(x) = 1 \iff \exists y \in \{ 0,1 \}^{r(|x|)}, \psi(x,y) = 1) \},
    \]
    and
    \[
        \mathsf \Pi_{k+1} = \{ \phi, \exists \psi \in \mathsf \Sigma_k, \exists r, (\phi(x) = 1 \iff \forall y \in \{ 0,1 \}^{r(|x|)}, \psi(x,y) = 1) \},
    \]
    or, maybe more clearly,
    \[
        \mathsf \Pi_{k+1} = \{ \phi, \exists \psi \in \mathsf \Sigma_k, \exists r, (\phi(x) = 0 \iff \exists y \in \{ 0,1 \}^{r(|x|)}, \psi(x,y) = 0) \}.
    \]
\end{definition}

In other words, the $ \mathsf \Sigma_{k+1} $'s and the $ \mathsf \Pi_{k+1} $'s are constructed from the $ \mathsf \Sigma_k $'s and the $ \mathsf \Pi_k $'s in the same way as $ \mathsf{NP} $ is constructed from $ \mathsf P $ and $ \mathsf{coNP} $ is constructed from $ \mathsf{coP} $.

\begin{definition}
    We also define, for all $ k \ge 0 $,
    \[
        \Delta_k = \mathsf \Sigma_k \cap \mathsf \Pi_k.
    \]
\end{definition}

This \og hierarchy \fg is called as such in the sense that :

\begin{proposition}
    For all $ k \ge 0 $, we have
    \[
        \mathsf \Sigma_k \subset \mathsf \Sigma_{k+1},
    \]
    and
    \[
        \mathsf \Pi_k \subset \mathsf \Pi_{k+1}.
    \]
\end{proposition}

\begin{proof}
    We have already shown that $ \mathsf P \subset \mathsf{NP} $, and that $ \mathsf{coP} \subset \mathsf{coNP} $ : this prooves $ \mathsf \Sigma_0 \subset \mathsf \Sigma_1 $, and $ \mathsf \Pi_0 \subset \mathsf \Pi_1 $.

    Let $ k \ge 0 $ and assume $ \mathsf \Sigma_k \subset \mathsf \Sigma_{k+1} $, as well as $ \mathsf \Pi_k \subset \mathsf \Pi_{k+1} $.

    Let $ \phi \in \mathsf \Sigma_{k+1} $. Then there exists $ \psi \in \mathsf \Pi_k $ such that the rest of the definition of $ \mathsf \Sigma_{k+1} $ holds. By assumption, we then have $ \psi \in \mathsf \Pi_{k+1} $, and the rest of the definition of $ \mathsf \Sigma_{k+2} $ is the same as in $ \mathsf \Sigma_{k+1} $ and does not depend on $ k $. So, $ \phi \in \mathsf \Sigma_{k+1} $.

    Similarly (or shall we say, symetrically), $ \mathsf \Sigma_k \subset \mathsf \Sigma_{k+1} $ yields $ \mathsf \Pi_{k+1} \subset \mathsf \Pi_{k+2} $. We have prooven the result.
\end{proof}

\begin{definition}[Polynomial hierarchy]
    We set
    \[
        \mathsf{PH} = \bigcup_{n=0}^\infty (\mathsf \Sigma_k \cup \mathsf \Pi_k).
    \]
    This class of problems is called the \textit{polynomial hierarchy}.
\end{definition}

Although we strongly believe that this hierarchy is a \textit{real} one, in the sense that each \og layer \fg actually brings new functions to the latter, no one was able to proove it to this day. In particular, we strongly believe that the polynomial hierarchy does not \textit{collapse}.

\begin{proposition}
    Let $ p \ge 0 $.

    If $ \mathsf \Sigma_p = \mathsf \Pi_p $, then
    \[
        \forall k \ge p, \mathsf \Sigma_k = \mathsf \Pi_k.
    \]
\end{proposition}

\begin{proof}
    Let $ p \ge 0 $ and assume that $ \Sigma $
\end{proof}

\subsubsection{The $ \mathsf{EXPTIME} $ and $ \mathsf{NEXPTIME} $ classes}

We will likely not discuss these classes too much, but here they are.

\begin{definition}[$ \mathsf{EXPTIME} $ and $ \mathsf{NEXPTIME} $]
    We say that $ \phi \in \mathsf{EXPTIME} $ if $ \phi $ can be computed in time at most $ \exp(P(|x|)) $, where $ P $ is some polynomial, using a deterministic Turing machine.

    We say that $ \phi \in \mathsf{NEXPTIME} $ if $ \phi $ can be computed in time at most $ \exp(P(|x|)) $, where $ P $ is some polynomial, using a non-deterministic Turing machine.
\end{definition}

The definition uses a polynomial $ P $ and a bound $ \exp(P(|x|)) $, because it is quite the only possibility that would not depend on the choice of an encoding for $ x $, and the choice of a definition for deterministic and non-deterministic Turing machines.

\subsubsection{The $ \mathsf{PSPACE} $ class}

If time is an important resource in algorithmics, so is space.

\begin{definition}[$ \mathsf{PSPACE} $]
    We say that $ \phi \in \mathsf{PSPACE} $ if there exists a polynomial $ P $ such that for any input $ x $, the head of the Turing machine does not go outside the slots of index in $ [-p(|x|),p(|x|)] $.
\end{definition}

In other words, the space (memory allocated) to treating the problem is polynomial in the size of the input. Ass we will see later, this class of problems contains many other ones, including the whole polynomial hierarchy !

\subsubsection{The $ \mathsf L $ and $ \mathsf{NL} $ classes}

As in \cite{gowers2024}, we do not dive into too much details in this definition.

\begin{definition}[$ \mathsf L $]
    The class $ \mathsf{L} $, or $ \mathsf{LOGSPACE} $, is the class of problems such that there exists a deterministic Turing machine with two tapes, one in \og reading-only \fg mode (in other words, the Turing machine cannot write on it) that contains the input $ x $, and a second one, on which the Turing machine will write on at most $ O(\log(|x|)) $ slots.
\end{definition}

In other words, these are problems that require, to solve, at any given time, an at most logarithmic space of memory. For instance, adding two numbers is in $ \mathsf{LOGSPACE} $ : that's why we can do this on paper with our brains !

It is necessary for this definition to consider a reading-only tape, because saying that \og $ \mathsf{LOGSPACE} $ is the class of problems that require an at most logarithmic amount of space to be treated \fg would be problematic because the input $ x $ itself requires more than $ O(\log(|x|)) $ space on the tape. For that reason, we set it separately on a read-only tape. In our analogy : it is easy for humans to add arbitrary sized-numbers on paper using their brain, because they do not require to know the whole of them at each step, only a small portion of it.

We will later proove that $ \mathsf L \subset \mathsf P $, but it is still an open problem to know whether or not $ \mathsf L = \mathsf P $.

\begin{definition}[$ \mathsf NL $]
    The class $ \mathsf{NL} $, or $ \mathsf{NLOGSPACE} $, is the class of problems such that there exists a non-deterministic Turing machine with two tapes, one in \og reading-only \fg mode (in other words, the Turing machine cannot write on it) that contains the input $ x $, and a second one, on which the Turing machine will write on at most $ O(\log(|x|)) $ slots.
\end{definition}

\subsubsection{The $ \mathsf{NPC} $ class}

\begin{definition}[$ \mathsf{NPH} $ problem]
    A problem $ \phi $ is said to be \textit{$ \mathsf{NP} $-hard} if
    \[
        \forall \psi \in \mathsf{NP}, \psi \prec \phi
    \]
    We denote $ \mathsf{NPH} $ the class of $ \mathsf{NP} $-hard problems.
\end{definition}

In other words, $ \mathsf{NP} $-hard problems are problems that are harder than any $ \mathsf{NP} $ problem. In other words... they are hard.

\begin{definition}[$ \mathsf{NP} $-completeness]
    A problem is said to be $ \mathsf{NP} $-complete if it is both $ \mathsf{NP} $ and $ \mathsf{NP} $-hard.

    We denote $ \mathsf{NPC} $ the class of $ \mathsf{NP} $-complete problems.
\end{definition}

That is, $ \mathsf{NP} $-complete problems are both hard and harder than any $ \mathsf{NP} $ problem. 

One quite astonishing thing about $ \mathsf{NPC} $ problems is that they exist. The first example that is given in most standard complexity theory lectures is always either $ \mathsf{SAT} $ or $ \mathsf{3SAT} $. We will later (in Section \ref{subsec:SATNP}) provide further discussion on $ \mathsf{NP} $-complete problems. In particular, we will proove that these problems are both $ \mathsf{NP} $-complete, and later on give more examples of $ \mathsf{NPC} $ problems.

\begin{remark}
    In \cite{gowers2024} are presented several other classes of complexity, but we have here already gone far too far away from our original goal of deep diving in combinatorial optimization. Let us mention them briefly.
    \begin{itemize}
        \item so
    \end{itemize}
\end{remark}

\subsection{Results and inclusions between problem complexity classes}

We start by the most important inclusion.

\begin{proposition}
    We have $ \mathsf P \subset \mathsf{NP} $.
\end{proposition}

\begin{proof}
    Considering the third definition of non-deterministic Turing machines, a deterministic Turing machine can be seen as a non-deterministic with a transition function $ \delta $ that only outputs singletons. As a result, deterministic Turing machines are a special case of non-deterministic ones. Consequently, $ \mathsf P \subset \mathsf{NP} $.
\end{proof}

This is an easy inclusion, the converse of which is astoninshingly hard --- it is, as we all know, one of the \textit{millenium problems}. Though, as we will see later, it is quite clear that $ \mathsf P \neq \mathsf{NP} $ is the only possible option.



\begin{corollary}
    If P = NP, then P = PH.
\end{corollary}

This  

\begin{proof}
    We apply the previous result for $ k = 0 $.
\end{proof}

\begin{proposition}
    We have NP $ \subset $ PSPACE.
\end{proposition}

\begin{proof}
    The idea is that we can compute all possibilities suggested by the non-deterministic Turing machine, one by one, and erase the answers one after the other.

    Recall that a $ \phi \in $ NP if and only if
    \[
        \exists \psi \in \mathrm P, \forall x \in \{ 0,1 \}^*, ( \phi(x) = 1 \iff \exists r \in \mathbf R[X], \exists y \in \{ 0,1 \}^{r(|x|)}, \psi(x,y) = 1)
    \]
    Because $ r $ is a polynomial in this definition, it is possible to do that in polynomial space.


Once $P$ and $NP$ have been defined,
\end{proof}

\subsection{$ \mathsf{NP} $ completeness}

\begin{theorem}
    
\end{theorem}

\begin{definition}

\end{definition}

% Si P = N P Alors .. .disjoints, ... ...